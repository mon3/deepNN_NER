{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "masters_elmo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon3/deepNN_NER/blob/master/masters_elmo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r5k3A6IBOlWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "52e9d6e9-2e16-48cf-c554-cd23b3c1965f"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 18.1MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.0MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.0MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.6MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.6MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 7.6MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 7.6MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 7.6MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 8.1MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.1MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.1MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.2MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.2MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.3MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.1MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 8.1MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 8.1MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 8.1MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 42.5MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 43.7MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 43.5MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 44.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 40.7MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 40.5MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 45.5MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 45.2MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 45.2MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 10.4MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 10.4MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 10.4MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 10.2MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 10.2MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 10.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 10.1MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 10.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 10.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 9.9MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 36.3MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 34.1MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 33.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 34.9MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 34.9MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 37.8MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 38.5MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 38.3MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 39.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 40.2MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 40.5MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 44.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 45.4MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 47.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 12.2MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 12.1MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 11.2MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 11.2MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 11.2MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 11.3MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 11.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 11.2MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 11.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 11.1MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 34.3MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 34.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 45.8MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 46.6MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 46.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 45.9MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 45.7MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 46.5MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 48.0MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 49.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 50.1MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 46.5MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 46.2MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 46.2MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 46.6MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.0MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 46.9MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.6MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 45.9MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 46.3MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 45.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 50.1MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 49.9MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 48.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 19.3MB/s \n",
            "\u001b[?25h  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QcK_l5YLyjok",
        "colab_type": "code",
        "outputId": "09902dca-8ca5-4901-de72-1df851b0ef59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyxR3hxEOnch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I6N-2GkA-l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_module = drive.CreateFile({'id': '1nEox1MVwcpP3Fu5RGJzhKi358x9jTA2h'})\n",
        "validation_module.GetContentFile('validation.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdmP1G75z_ED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module3 = drive.CreateFile({'id':'1v9lb4SlclxAAKYjIRk0lrYp5oP3WEU3J'})\n",
        "prepro_elmo_module3.GetContentFile('elmo_preprocess_sentences_nontrain.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yzjtx0HjGRHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0598bc5f-cb18-424c-ff67-ec4cfce4ecf4"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate, Lambda\n",
        "# from elmo_prod import createMatrices, createBatches, createBatchesNonTrain, iterate_minibatches, readfile, addCharInformation, padding\n",
        "\n",
        "from elmo_preprocess_sentences_nontrain import createMatrices, createBatches, createBatchesNonTrain, iterate_minibatches, readfile, addCharInformation, padding\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0407 09:19:17.633556 139906490656640 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1J6puMBBYJ4",
        "colab_type": "code",
        "outputId": "8fd78741-bd2e-4b0d-94ae-09e161ad3204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        self.char_embedding_size = CHAR_EMBEDDING\n",
        "        self.version = VERSION\n",
        "        self.embedding = EMBEDDING\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(os.path.join(LOCATION_POINTER, \"data/train.txt\"))\n",
        "        self.devSentences = readfile(os.path.join(LOCATION_POINTER, \"data/dev.txt\"))\n",
        "        self.testSentences = readfile(os.path.join(LOCATION_POINTER, \"data/test.txt\"))\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "                    \n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        # creates identity matrix for token cases\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        word2Idx = {}\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx) # 2,3,4 ...\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}  # index to label(reverted)\n",
        "                                                                \n",
        "                                                                \n",
        "    def createBatches(self):\n",
        "        \n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        \n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i in dataset.keys():\n",
        "          key = str(i)\n",
        "          if int(i) == 1:\n",
        "            continue\n",
        "\n",
        "          data = dataset[key]\n",
        "          tokens = []\n",
        "          caseing = []\n",
        "          char = []\n",
        "          tokens_length = []\n",
        "          labels = []\n",
        "          \n",
        "          for dt in data:\n",
        "            ############################# ADDED ################################\n",
        "            tokens_in = []\n",
        "            caseing_in = []\n",
        "            char_in = []\n",
        "            labels_in = []\n",
        "            ############################# ADDED ################################\n",
        "\n",
        "            t, c, ch, l, _ = dt\n",
        "            t = np.expand_dims(t, -1)\n",
        "#             l = np.expand_dims(l, -1)  # bo tak jest robione z danymi do trenowania modelu\n",
        "\n",
        "            tokens.append(t)\n",
        "            caseing.append(c)\n",
        "            char.append(ch)\n",
        "            labels.append(l)\n",
        "            \n",
        "            tokens_in = np.asarray(tokens)\n",
        "            caseing_in = np.asarray(caseing)\n",
        "            char_in = np.asarray(char)\n",
        "            labels_in = np.asarray(labels)\n",
        "         \n",
        "            pred = model.predict([tokens_in, caseing_in, char_in], verbose=False)[0]\n",
        "#             print(\"PRED: \".format(pred))\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "#             print(\"Correct labels: {}\".format(l))\n",
        "#             print(\"Predict labels: {}\".format(pred))\n",
        "#             print(\"Predict labels: {}\".format(pred.tolist()))\n",
        "\n",
        "            correctLabels.append(l) ## maybe labels\n",
        "            predLabels.append(pred.tolist())  # pred converted to list - to \n",
        "            # preserve the same format as correct labels (list of ints)\n",
        "       \n",
        "          \n",
        "        return predLabels, correctLabels\n",
        "      \n",
        "    \n",
        "    def ELMoEmbedding(self, tokens_input):\n",
        "        \"\"\" Return elmo embedding using tf_hub \"\"\"\n",
        "        elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "        # signature points to the purpose of why we would like to use the modules\n",
        "        # as_dict=True needed to output word embeddings instead of defualtss\n",
        "        print(\"Elmo shape: {} - {}\".format(tokens_input.shape[0], tokens_input.shape[1]))\n",
        "\n",
        "        return elmo_model(tf.squeeze(tf.cast(tokens_input, tf.string), axis=1), signature=\"default\", as_dict=True)['elmo']\n",
        "  \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")  #input N sentences, each 52 chras length\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx),self.char_embedding_size, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)  # pool_size=52: max sentence length\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        # input of N-dimensional vectors (1st arg in shape points to the size of input vectors)\n",
        "        words_input = Input(shape=(None, ), dtype=\"string\", name='words_input')\n",
        "        words = Lambda(self.ELMoEmbedding, output_shape=(None, 1024))(words_input)\n",
        "\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)  ## trainable=False since we provide case embeddings\n",
        "       \n",
        "        output = concatenate([words, casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        for layer in self.model.layers:\n",
        "            print(\"Layer {}: {}\".format(layer.name, layer.output_shape))\n",
        "        \n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file=os.path.join(LOCATION_POINTER, 'model_{}.png'.format(self.version)))\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            # batch: [word_indices, case_indices, char_indices]\n",
        "            print(\"Batch len: {}\".format(self.train_batch_len))\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch, self.train_batch_len)):\n",
        "                labels, tokens, casing, char, self.tokens_length = batch  \n",
        "#                 # ToDO: verify if needed!!! \n",
        "#                 if len(tokens) <= 1:\n",
        "#                   print(\"Tokens removed: {}\".format(tokens))\n",
        "#                   continue\n",
        "                self.model.train_on_batch([tokens, casing, char], labels)\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            print(\"PRED LABELS: {}\".format(predLabels[:5]))\n",
        "            print(\"CORRECT LABELS: {}\".format(correctLabels[:5]))\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.char_embedding_size,\n",
        "                                                        self.optimizer.__class__.__name__,\n",
        "                                                        self.version,\n",
        "                                                        self.embedding\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(os.path.join(LOCATION_POINTER, modelName))\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = os.path.join(LOCATION_POINTER, self.modelName + \".txt\")\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "        \n",
        "    def saveResults(self):\n",
        "        plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "        plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"F1 score\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(LOCATION_POINTER, self.modelName + \".png\"))\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xdX6Eqh-BfKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 80               # paper: 80\n",
        "DROPOUT = 0.68           # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.0  # not specified in paper, 0.25 recommended; in other papers: 0.0\n",
        "LSTM_STATE_SIZE = 275    # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n",
        "LOCATION_POINTER = '/content/gdrive/My Drive/masters_thesis/'\n",
        "CHAR_EMBEDDING = 30   # paper: 25, previously: 30\n",
        "VERSION = 25\n",
        "EMBEDDING = \"elmo\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZP5gZkj6R8Kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "A0_c8WQ2BnBx",
        "colab_type": "code",
        "outputId": "100fce09-cab3-43f2-f8f2-17f1732bb25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()\n",
        "cnn_blstm.saveResults()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0407 09:20:10.533504 139906490656640 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Elmo shape: ? - ?\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0407 09:20:20.002110 139906490656640 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Layer Character_input: (None, None, 52)\n",
            "Layer Character_embedding: (None, None, 52, 30)\n",
            "Layer dropout_1: (None, None, 52, 30)\n",
            "Layer Convolution: (None, None, 52, 30)\n",
            "Layer Maxpool: (None, None, 1, 30)\n",
            "Layer words_input: (None, None)\n",
            "Layer casing_input: (None, None)\n",
            "Layer Flatten: (None, None, 30)\n",
            "Layer lambda_1: (None, None, 1024)\n",
            "Layer embedding_2: (None, None, 8)\n",
            "Layer dropout_2: (None, None, 30)\n",
            "Layer concatenate_1: (None, None, 1062)\n",
            "Layer BLSTM: (None, None, 550)\n",
            "Layer Softmax_layer: (None, None, 9)\n",
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0407 09:20:24.019344 139906490656640 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0704\n",
            "f1 dev  0.0792 \n",
            "\n",
            "Epoch 1/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0779\n",
            "f1 dev  0.0811 \n",
            "\n",
            "Epoch 2/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0794\n",
            "f1 dev  0.0807 \n",
            "\n",
            "Epoch 3/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0792\n",
            "f1 dev  0.0824 \n",
            "\n",
            "Epoch 4/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0805\n",
            "f1 dev  0.083 \n",
            "\n",
            "Epoch 5/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.079\n",
            "f1 dev  0.0831 \n",
            "\n",
            "Epoch 6/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "PRED LABELS: [[0, 3], [0, 3], [0, 3], [0, 3], [0, 3]]\n",
            "CORRECT LABELS: [[0, 3], [8, 5], [5, 5], [8, 5], [8, 5]]\n",
            "f1 test  0.0802\n",
            "f1 dev  0.0823 \n",
            "\n",
            "Epoch 7/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}