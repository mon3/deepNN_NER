{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmo_test_new.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon3/deepNN_NER/blob/master/elmo_test_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r5k3A6IBOlWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcK_l5YLyjok",
        "colab_type": "code",
        "outputId": "576ebace-d94c-4349-d4fe-12cd77ca5911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyxR3hxEOnch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPzHW796AQau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_module = drive.CreateFile({'id':'1lBy0_BsIQ_gzuCkM-P4MWbstIZFkHj8L'})\n",
        "your_module.GetContentFile('preprocess.py')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL1sOrhjkcdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module = drive.CreateFile({'id':'1S_fLdw31wQMBtIqIzEIelYlLim4A8dxA'})\n",
        "prepro_elmo_module.GetContentFile('elmo_preprocessing1.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRqpT7ciZY5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module1 = drive.CreateFile({'id':'1jK137fvsVr_DO7UiEA9J0Qbx_LLm7XPS'})\n",
        "prepro_elmo_module1.GetContentFile('elmo_prep11.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I6N-2GkA-l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_module = drive.CreateFile({'id': '1nEox1MVwcpP3Fu5RGJzhKi358x9jTA2h'})\n",
        "validation_module.GetContentFile('validation.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb4L3wr2jq4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module2 = drive.CreateFile({'id':'1gmJJScIBfWmOdDMzOJO2kZvHb-r0zJpG'})\n",
        "prepro_elmo_module2.GetContentFile('elmo_prep4.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdmP1G75z_ED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module3 = drive.CreateFile({'id':'1v9lb4SlclxAAKYjIRk0lrYp5oP3WEU3J'})\n",
        "prepro_elmo_module3.GetContentFile('elmo_preprocess_sentences_nontrain.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yzjtx0HjGRHv",
        "colab_type": "code",
        "outputId": "f62e97e1-a5ed-4f93-ae9a-45a20aecc01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate, Lambda\n",
        "#from preprocess import readfile, addCharInformation, padding\n",
        "from elmo_preprocess_sentences_nontrain import createMatrices, createBatches, createBatchesNonTrain, iterate_minibatches, readfile, addCharInformation, padding\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0404 11:59:22.591709 140290160236416 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xdX6Eqh-BfKz",
        "colab_type": "code",
        "outputId": "d7e2f74a-00d0-49ff-d6bf-1093e49b207e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 80               # paper: 80\n",
        "DROPOUT = 0.68           # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.0  # not specified in paper, 0.25 recommended; in other papers: 0.0\n",
        "LSTM_STATE_SIZE = 275    # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n",
        "LOCATION_POINTER = '/content/gdrive/My Drive/masters_thesis/'\n",
        "CHAR_EMBEDDING = 30   # paper: 25, previously: 30\n",
        "VERSION = 16\n",
        "EMBEDDING = \"elmo\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0404 11:59:27.612550 140290160236416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1J6puMBBYJ4",
        "colab_type": "code",
        "outputId": "64902e7c-9b2d-4276-c425-7246630d58b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        self.char_embedding_size = CHAR_EMBEDDING\n",
        "        self.version = VERSION\n",
        "        self.embedding = EMBEDDING\n",
        "#         self.tokens_length = []\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(os.path.join(LOCATION_POINTER, \"data/train.txt\"))\n",
        "        self.devSentences = readfile(os.path.join(LOCATION_POINTER, \"data/dev.txt\"))\n",
        "        self.testSentences = readfile(os.path.join(LOCATION_POINTER, \"data/test.txt\"))\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "                    \n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        # creates identity matrix for token cases\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "#         self.wordEmbeddings = []\n",
        "        \n",
        "#         word represented as 50-dim vector\n",
        "#         ToDO: test with 300-dim vectors (GloVE 42B, GloVE 84B)\n",
        "#         if self.embedding == \"fastText\":\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/wiki-news-300d-1M.vec\"), encoding=\"utf-8\")\n",
        "#         else:\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/glove.6B.50d.txt\"), encoding=\"utf-8\")\n",
        "\n",
        "#         # loop through each word in embeddings\n",
        "#         for i, line in enumerate(fEmbeddings):\n",
        "#             if i==0 and self.embedding == \"fastText\":\n",
        "#                 continue\n",
        "                \n",
        "#             split = line.strip().split(\" \") # removes leading and trailing chars and splits into list of single values\n",
        "#             word = split[0]  # embedding word entry\n",
        "\n",
        "#             if len(word2Idx) == 0:  # add padding+unknown\n",
        "#                 word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#                 word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.random.uniform(-0.25, 0.25, len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#             if split[0].lower() in words:\n",
        "#                 vector = np.array([float(num) for num in split[1:]])\n",
        "#                 self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "#                 word2Idx[split[0]] = len(word2Idx)  # corresponding word dict; increments by 1 for each word\n",
        "\n",
        "#         self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx) # 2,3,4 ...\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}  # index to label(reverted)\n",
        "                                                                \n",
        "                                                                \n",
        "    def createBatches(self):\n",
        "        \n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "#         self.dev_batch, self.dev_batch_len = createBatchesNonTrain(self.dev_set)\n",
        "#         self.test_batch, self.test_batch_len = createBatchesNonTrain(self.test_set)\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        \n",
        "        #nTODO; check if labels are numeric\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i in dataset.keys():\n",
        "          key = str(i)\n",
        "          print(\"I: {}\".format(i))\n",
        "          if int(i) == 1:\n",
        "            continue\n",
        "          print(\"I after: {}\".format(i))\n",
        "\n",
        "          data = dataset[key]\n",
        "          tokens = []\n",
        "          tl = []\n",
        "          caseing = []\n",
        "          char = []\n",
        "          tokens_length = []\n",
        "          labels = []\n",
        "          for dt in data:\n",
        "#             print(\"DT: {}\".format(dt))\n",
        "            t, c, ch, l, _ = dt\n",
        "#             if len(c)==1 or len(t)==1:\n",
        "#               continue\n",
        "              \n",
        "#             print(\"Single token: {}-{}\".format(len(c), t))\n",
        "#             print(\"Single labels: {}\".format(l))\n",
        "#             if len(t)<=1:\n",
        "# #               continue\n",
        "#             print(\"T: {}\".format(t))\n",
        "#             print(t.split(' '))\n",
        "#             t = t.split(' ')  # moze tego nie trzeba\n",
        "          \n",
        "            t = np.expand_dims(t, -1)\n",
        "            l = np.expand_dims(l, -1)\n",
        "#             t = tf.reshape(t, [-1])\n",
        "      \n",
        "#             t = np.expand_dims(t.split(' '), -1)\n",
        "#             c = np.expand_dims(c, -1)\n",
        "            tokens.append(t)\n",
        "            caseing.append(c)\n",
        "            char.append(ch)\n",
        "            labels.append(l)\n",
        "            tokens_length.append(tl)\n",
        "            \n",
        "          tokens_in = np.asarray(tokens)\n",
        "#           if tokens_in.shape[0] <=1 or tokens_in.shape[1]<=1:\n",
        "#             continue\n",
        "          caseing_in = np.asarray(caseing)\n",
        "          char_in = np.asarray(char)\n",
        "#           print(\"Labels: {}\".format(labels))\n",
        "          labels_in = np.asarray(labels)\n",
        "#           print(\"Labels in: {}\".format(labels_in))\n",
        "          tokens_len_in = np.asarray(tokens_length)\n",
        "#           print(\"TOKENS_IN: {}\".format(tokens_in))\n",
        "#           print(\"LABELS_IN: {}\".format(labels_in))\n",
        "        \n",
        "#           print(\"TAG Tokens size: {}\".format(tokens_in.shape))\n",
        "#           if tokens_in.shape == (1,1):\n",
        "#             print(\"Tokens shape (1,1): {}\".format(tokens_in))\n",
        "#             continue\n",
        "#           print(\"TAG Labels size: {}\".format(labels_in.shape))\n",
        "#           print(\"TAG Caseing size: {}\".format(caseing_in.shape))\n",
        "#           print(\"Tokens: {}\".format(tokens_in))\n",
        "\n",
        "\n",
        "######################## TUTAJ ######################33\n",
        "          # daje shape (1,1) i się wykrzacza dla arraya z jednym zdaniem\n",
        "        \n",
        "  \n",
        "          print(\"Shapes test: {} {} {}\".format(tokens_in.shape, caseing_in.shape, char_in.shape))\n",
        "          if tokens_in.shape[0] <=1 or tokens_in.shape[1] <=1:\n",
        "              continue\n",
        "          pred = model.predict([tokens_in, caseing_in, char_in], verbose=False)[0]\n",
        "          pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "          correctLabels.append(labels)\n",
        "          predLabels.append(pred)\n",
        "          \n",
        "          \n",
        "        return predLabels, correctLabels\n",
        "      \n",
        "      \n",
        "      \n",
        "#         for i, data in enumerate(dataset):\n",
        "#             tokens, casing, char, labels, _ = data\n",
        "#             if len(tokens)<=1:\n",
        "#               continue\n",
        "#             t = np.expand_dims(tokens, -1)\n",
        "#             tokens = np.asarray(t)\n",
        "\n",
        "#             # print(\"TOKEN: {}\".format(t))\n",
        "\n",
        "#             l = np.expand_dims(labels, -1)\n",
        "#             labels = np.asarray(l)\n",
        "            \n",
        "            \n",
        "# #             tokens = np.asarray([tokens])\n",
        "#             print(\"TAG TOKENS: {}\".format(tokens))\n",
        "#             casing = np.asarray([casing])\n",
        "#             char = np.asarray([char])\n",
        "#             pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "#             pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "#             correctLabels.append(labels)\n",
        "#             predLabels.append(pred)\n",
        "#         return predLabels, correctLabels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         correctLabels = []\n",
        "#         predLabels = []\n",
        "#         # ToDO: only training data as sentences\n",
        "#         # ToDO: change for iteration through dict!\n",
        "#         for i in dataset.keys():\n",
        "#           key = str(i)\n",
        "#           data = dataset[key]\n",
        "#           for dt in data:\n",
        "#             ################################################################\n",
        "#             # sentences or tokens here!?\n",
        "#             ################################################################\n",
        "#             tokens, casing, char, labels, _ = dt\n",
        "# #             tokens = np.asarray([tokens])\n",
        "#             casing = np.asarray([casing])\n",
        "#             char = np.asarray([char])\n",
        "#             print(\"TAGGED tokens: {}\".format(tokens))\n",
        "#             pred = model.predict([tokens, casing, char], verbose=False, batch_size=len(tokens))[0]\n",
        "#             pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "#             correctLabels.append(labels)\n",
        "#             predLabels.append(pred)\n",
        "#         return predLabels, correctLabels\n",
        "    \n",
        "    \n",
        "\n",
        "    def ELMoEmbedding(self, tokens_input):\n",
        "        # ToDO: pojedyncze zdanie na inpucie - lista słów (stringi)\n",
        "        #       zdania na inpucie - lista list \n",
        "        # zdefiniować input jako tokeny!\n",
        "        \"\"\" Return elmo embedding using tf_hub \"\"\"\n",
        "        elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "        # signature points to the purpose of why we would like to use the modules\n",
        "        # as_dict=True needed to output word embeddings instead of defualtss\n",
        "#         return elmo_model(tf.cast(tokens_input, tf.string), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "#         return elmo_model(tf.cast(tokens_input, dtype=tf.string), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "#         return elmo_model(tf.reshape(tf.cast(tokens_input, tf.string), [-1]), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "        return elmo_model(tf.squeeze(tf.cast(tokens_input, tf.string)), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "#         return elmo_model(tf.cast(tokens_input, tf.string), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "#         return elmo_model(inputs={\"tokens\": tokens_input, \"sequence_len\": tokens_length}, signature=\"tokens\", as_dict=True)[\"word_emb\"]\n",
        "#       return elmo_model(x, signature=\"default\", as_dict=True)[\"word_emb\"]\n",
        "  \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")  #input N sentences, each 52 chras length\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx),self.char_embedding_size, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)  # pool_size=52: max sentence length\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        # input of N-dimensional vectors (1st arg in shape points to the size of input vectors)\n",
        "        words_input = Input(shape=(None, ), dtype=tf.string, name='words_input')\n",
        "        # model takes first dimension as number of examplese and second as the size of each example\n",
        "#         tokens_length = Input(shape=(None, None, ), name='tokens_length_input')  # co z data type\n",
        "#         print(\"After words input\")\n",
        "        # ToDO: Lambda powinna zwrócić odpowiedni rozmiar!\n",
        "#         words = self.ELMoEmbedding(words_input, tokens_length)\n",
        "        words = Lambda(self.ELMoEmbedding, output_shape=(None, 1024))(words_input)\n",
        "#         words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "#         words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self .wordEmbeddings],\n",
        "#                           trainable=False)(words_input)  # trainable=False since we provide word embeddings\n",
        "#         print(\"After: {}\".format(words))\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)  ## trainable=False since we provide case embeddings\n",
        "       \n",
        "      \n",
        "#         print(\"SIZES FOR CONCATENATION: {} {} {}\".format(words.shape, char.shape, casing.shape))\n",
        "        # concat & BLSTM\n",
        "#         print(\"DIMENSIONS: {}\".format(tf.shape(words_input)))\n",
        "              \n",
        "        output = concatenate([words, casing, char])\n",
        "#         output = concatenate([tf.reshape(words, \n",
        "#                                   [len(words), 52, 1024]), casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        for layer in self.model.layers:\n",
        "            print(\"Layer {}: {}\".format(layer.name, layer.output_shape))\n",
        "        \n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file=os.path.join(LOCATION_POINTER, 'model_{}.png'.format(self.version)))\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            # batch: [word_indices, case_indices, char_indices]\n",
        "            print(\"Batch len: {}\".format(self.train_batch_len))\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch, self.train_batch_len)):\n",
        "                labels, tokens, casing, char, self.tokens_length = batch  \n",
        "#                 tokens = np.expand_dims(tokens[0], 1)\n",
        "#                 print(tokens)\n",
        "#                 print(\"length: {}\".format(len(tokens)))\n",
        "#                 print(\"TOKENS: {}\".format(tokens))\n",
        "#                 tokens_in = tf.reshape(tokens, [-1])\n",
        "#                 print(labels)\n",
        "#                 print(tokens[:5])\n",
        "#                 print(\"before train on batch - i: {}\".format(i))\n",
        "#                 print(\"Iteration - tokens - {} - {}\".format(len(tokens), tokens))\n",
        "\n",
        "#                 # ToDO: verify if needed!!! \n",
        "                if len(tokens) <= 1:\n",
        "                  print(\"Tokens removed: {}\".format(tokens))\n",
        "                  continue\n",
        "#                 # ToDO: verify if needed!!! \n",
        "\n",
        "#                 if i==55 or i ==2:\n",
        "#                   print(len(tokens), len(labels))\n",
        "#                 print(\"Shapes: {} {} {}\".format(tokens.shape, casing.shape, char.shape))\n",
        "                self.model.train_on_batch([tokens, casing, char], labels)\n",
        "#                 print(\"after train on batch - i: {}\".format(i))\n",
        "\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            print(\"PRED LABELS: {}\".format(predLabels))\n",
        "            print(\"CORRECT LABELS: {}\".format(correctLabels))\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.char_embedding_size,\n",
        "                                                        self.optimizer.__class__.__name__,\n",
        "                                                        self.version,\n",
        "                                                        self.embedding\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(os.path.join(LOCATION_POINTER, modelName))\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = os.path.join(LOCATION_POINTER, self.modelName + \".txt\")\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "        \n",
        "    def saveResults(self):\n",
        "        plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "        plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"F1 score\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(LOCATION_POINTER, self.modelName + \".png\"))\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0_c8WQ2BnBx",
        "colab_type": "code",
        "outputId": "23a649d6-64b7-401e-8207-afdf4a416470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8345
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()\n",
        "cnn_blstm.saveResults()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0404 12:02:56.410142 140290160236416 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Layer Character_input: (None, None, 52)\n",
            "Layer Character_embedding: (None, None, 52, 30)\n",
            "Layer dropout_3: (None, None, 52, 30)\n",
            "Layer Convolution: (None, None, 52, 30)\n",
            "Layer Maxpool: (None, None, 1, 30)\n",
            "Layer words_input: (None, None)\n",
            "Layer casing_input: (None, None)\n",
            "Layer Flatten: (None, None, 30)\n",
            "Layer lambda_2: (None, None, 1024)\n",
            "Layer embedding_4: (None, None, 8)\n",
            "Layer dropout_4: (None, None, 30)\n",
            "Layer concatenate_2: (None, None, 1062)\n",
            "Layer BLSTM: (None, None, 550)\n",
            "Layer Softmax_layer: (None, None, 9)\n",
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "Tokens removed: [['\" one of the police versions in the case of the murder of young gypsy woman in topolcany , western slovakia , this july , is a suspicion that mark dutroux could have been involved in the murder , \" gajdos said without elaborating on the age of the victim and on the other versions .']]\n",
            "Tokens removed: [['compared with the end of last year , when t&n predicted a sluggish first half and a rebound later in 1996 , hope said : \" i think the difference ( now ) is the first half has not actually been as bad as some felt it was going to be , but equally we \\'re certainly not predicting a recovery in the second half . \"']]\n",
            "Tokens removed: [['15 - christian cullen , 14 - jeff wilson , 13 - walter little , 12 - frank bunce , 11 - glen osborne ; 10 - andrew mehrtens , 9 - justin marshall ; 8 - zinzan brooke , 7 - josh kronfeld , 6 - michael jones , 5 - ian jones , 4 - robin brooke , 3 - olo brown , 2 - sean fitzpatrick ( captain ) , 1 - craig dowd .']]\n",
            "Tokens removed: [['market talk - usda net change in weekly export commitments for the week ended august 22 , includes old crop and new crop , were : wheat up 595,400 tonnes old , nil new ; corn up 1,900 old , up 319,600 new ; soybeans down 12,300 old , up 300,800 new ; upland cotton up 50,400 bales new , nil old ; soymeal 54,800 old , up 100,600 new , soyoil nil old , up 75,000 new ; barley up 1,700 old , nil new ; sorghum 6,200 old , up 156,700 new ; pima cotton up 4,000 bales old , nil new ; rice up 49,900 old , nil new ...']]\n",
            "I: 1\n",
            "I: 2\n",
            "I after: 2\n",
            "Shapes test: (277, 1) (277, 2) (277, 2, 52)\n",
            "I: 3\n",
            "I after: 3\n",
            "Shapes test: (164, 1) (164, 3) (164, 3, 52)\n",
            "I: 4\n",
            "I after: 4\n",
            "Shapes test: (212, 1) (212, 4) (212, 4, 52)\n",
            "I: 5\n",
            "I after: 5\n",
            "Shapes test: (188, 1) (188, 5) (188, 5, 52)\n",
            "I: 6\n",
            "I after: 6\n",
            "Shapes test: (177, 1) (177, 6) (177, 6, 52)\n",
            "I: 7\n",
            "I after: 7\n",
            "Shapes test: (336, 1) (336, 7) (336, 7, 52)\n",
            "I: 8\n",
            "I after: 8\n",
            "Shapes test: (327, 1) (327, 8) (327, 8, 52)\n",
            "I: 9\n",
            "I after: 9\n",
            "Shapes test: (223, 1) (223, 9) (223, 9, 52)\n",
            "I: 10\n",
            "I after: 10\n",
            "Shapes test: (127, 1) (127, 10) (127, 10, 52)\n",
            "I: 11\n",
            "I after: 11\n",
            "Shapes test: (80, 1) (80, 11) (80, 11, 52)\n",
            "I: 12\n",
            "I after: 12\n",
            "Shapes test: (63, 1) (63, 12) (63, 12, 52)\n",
            "I: 13\n",
            "I after: 13\n",
            "Shapes test: (54, 1) (54, 13) (54, 13, 52)\n",
            "I: 14\n",
            "I after: 14\n",
            "Shapes test: (55, 1) (55, 14) (55, 14, 52)\n",
            "I: 15\n",
            "I after: 15\n",
            "Shapes test: (58, 1) (58, 15) (58, 15, 52)\n",
            "I: 16\n",
            "I after: 16\n",
            "Shapes test: (42, 1) (42, 16) (42, 16, 52)\n",
            "I: 17\n",
            "I after: 17\n",
            "Shapes test: (52, 1) (52, 17) (52, 17, 52)\n",
            "I: 18\n",
            "I after: 18\n",
            "Shapes test: (50, 1) (50, 18) (50, 18, 52)\n",
            "I: 19\n",
            "I after: 19\n",
            "Shapes test: (46, 1) (46, 19) (46, 19, 52)\n",
            "I: 20\n",
            "I after: 20\n",
            "Shapes test: (54, 1) (54, 20) (54, 20, 52)\n",
            "I: 21\n",
            "I after: 21\n",
            "Shapes test: (66, 1) (66, 21) (66, 21, 52)\n",
            "I: 22\n",
            "I after: 22\n",
            "Shapes test: (47, 1) (47, 22) (47, 22, 52)\n",
            "I: 23\n",
            "I after: 23\n",
            "Shapes test: (52, 1) (52, 23) (52, 23, 52)\n",
            "I: 24\n",
            "I after: 24\n",
            "Shapes test: (39, 1) (39, 24) (39, 24, 52)\n",
            "I: 25\n",
            "I after: 25\n",
            "Shapes test: (50, 1) (50, 25) (50, 25, 52)\n",
            "I: 26\n",
            "I after: 26\n",
            "Shapes test: (49, 1) (49, 26) (49, 26, 52)\n",
            "I: 27\n",
            "I after: 27\n",
            "Shapes test: (28, 1) (28, 27) (28, 27, 52)\n",
            "I: 28\n",
            "I after: 28\n",
            "Shapes test: (38, 1) (38, 28) (38, 28, 52)\n",
            "I: 29\n",
            "I after: 29\n",
            "Shapes test: (49, 1) (49, 29) (49, 29, 52)\n",
            "I: 30\n",
            "I after: 30\n",
            "Shapes test: (40, 1) (40, 30) (40, 30, 52)\n",
            "I: 31\n",
            "I after: 31\n",
            "Shapes test: (49, 1) (49, 31) (49, 31, 52)\n",
            "I: 32\n",
            "I after: 32\n",
            "Shapes test: (29, 1) (29, 32) (29, 32, 52)\n",
            "I: 33\n",
            "I after: 33\n",
            "Shapes test: (46, 1) (46, 33) (46, 33, 52)\n",
            "I: 34\n",
            "I after: 34\n",
            "Shapes test: (25, 1) (25, 34) (25, 34, 52)\n",
            "I: 35\n",
            "I after: 35\n",
            "Shapes test: (26, 1) (26, 35) (26, 35, 52)\n",
            "I: 36\n",
            "I after: 36\n",
            "Shapes test: (22, 1) (22, 36) (22, 36, 52)\n",
            "I: 37\n",
            "I after: 37\n",
            "Shapes test: (27, 1) (27, 37) (27, 37, 52)\n",
            "I: 38\n",
            "I after: 38\n",
            "Shapes test: (32, 1) (32, 38) (32, 38, 52)\n",
            "I: 39\n",
            "I after: 39\n",
            "Shapes test: (15, 1) (15, 39) (15, 39, 52)\n",
            "I: 40\n",
            "I after: 40\n",
            "Shapes test: (13, 1) (13, 40) (13, 40, 52)\n",
            "I: 41\n",
            "I after: 41\n",
            "Shapes test: (11, 1) (11, 41) (11, 41, 52)\n",
            "I: 42\n",
            "I after: 42\n",
            "Shapes test: (11, 1) (11, 42) (11, 42, 52)\n",
            "I: 43\n",
            "I after: 43\n",
            "Shapes test: (11, 1) (11, 43) (11, 43, 52)\n",
            "I: 44\n",
            "I after: 44\n",
            "Shapes test: (10, 1) (10, 44) (10, 44, 52)\n",
            "I: 45\n",
            "I after: 45\n",
            "Shapes test: (11, 1) (11, 45) (11, 45, 52)\n",
            "I: 46\n",
            "I after: 46\n",
            "Shapes test: (2, 1) (2, 46) (2, 46, 52)\n",
            "I: 47\n",
            "I after: 47\n",
            "Shapes test: (5, 1) (5, 47) (5, 47, 52)\n",
            "I: 48\n",
            "I after: 48\n",
            "Shapes test: (5, 1) (5, 48) (5, 48, 52)\n",
            "I: 49\n",
            "I after: 49\n",
            "Shapes test: (3, 1) (3, 49) (3, 49, 52)\n",
            "I: 50\n",
            "I after: 50\n",
            "Shapes test: (4, 1) (4, 50) (4, 50, 52)\n",
            "I: 53\n",
            "I after: 53\n",
            "Shapes test: (4, 1) (4, 53) (4, 53, 52)\n",
            "I: 54\n",
            "I after: 54\n",
            "Shapes test: (1, 1) (1, 54) (1, 54, 52)\n",
            "I: 55\n",
            "I after: 55\n",
            "Shapes test: (4, 1) (4, 55) (4, 55, 52)\n",
            "I: 56\n",
            "I after: 56\n",
            "Shapes test: (1, 1) (1, 56) (1, 56, 52)\n",
            "I: 59\n",
            "I after: 59\n",
            "Shapes test: (1, 1) (1, 59) (1, 59, 52)\n",
            "I: 60\n",
            "I after: 60\n",
            "Shapes test: (1, 1) (1, 60) (1, 60, 52)\n",
            "I: 64\n",
            "I after: 64\n",
            "Shapes test: (1, 1) (1, 64) (1, 64, 52)\n",
            "I: 66\n",
            "I after: 66\n",
            "Shapes test: (1, 1) (1, 66) (1, 66, 52)\n",
            "I: 69\n",
            "I after: 69\n",
            "Shapes test: (1, 1) (1, 69) (1, 69, 52)\n",
            "I: 71\n",
            "I after: 71\n",
            "Shapes test: (1, 1) (1, 71) (1, 71, 52)\n",
            "I: 72\n",
            "I after: 72\n",
            "Shapes test: (1, 1) (1, 72) (1, 72, 52)\n",
            "I: 77\n",
            "I after: 77\n",
            "Shapes test: (1, 1) (1, 77) (1, 77, 52)\n",
            "I: 80\n",
            "I after: 80\n",
            "Shapes test: (1, 1) (1, 80) (1, 80, 52)\n",
            "I: 124\n",
            "I after: 124\n",
            "Shapes test: (1, 1) (1, 124) (1, 124, 52)\n",
            "PRED LABELS: []\n",
            "CORRECT LABELS: []\n",
            "f1 test  0\n",
            "I: 1\n",
            "I: 2\n",
            "I after: 2\n",
            "Shapes test: (265, 1) (265, 2) (265, 2, 52)\n",
            "I: 3\n",
            "I after: 3\n",
            "Shapes test: (96, 1) (96, 3) (96, 3, 52)\n",
            "I: 4\n",
            "I after: 4\n",
            "Shapes test: (173, 1) (173, 4) (173, 4, 52)\n",
            "I: 5\n",
            "I after: 5\n",
            "Shapes test: (130, 1) (130, 5) (130, 5, 52)\n",
            "I: 6\n",
            "I after: 6\n",
            "Shapes test: (132, 1) (132, 6) (132, 6, 52)\n",
            "I: 7\n",
            "I after: 7\n",
            "Shapes test: (302, 1) (302, 7) (302, 7, 52)\n",
            "I: 8\n",
            "I after: 8\n",
            "Shapes test: (224, 1) (224, 8) (224, 8, 52)\n",
            "I: 9\n",
            "I after: 9\n",
            "Shapes test: (130, 1) (130, 9) (130, 9, 52)\n",
            "I: 10\n",
            "I after: 10\n",
            "Shapes test: (123, 1) (123, 10) (123, 10, 52)\n",
            "I: 11\n",
            "I after: 11\n",
            "Shapes test: (104, 1) (104, 11) (104, 11, 52)\n",
            "I: 12\n",
            "I after: 12\n",
            "Shapes test: (83, 1) (83, 12) (83, 12, 52)\n",
            "I: 13\n",
            "I after: 13\n",
            "Shapes test: (57, 1) (57, 13) (57, 13, 52)\n",
            "I: 14\n",
            "I after: 14\n",
            "Shapes test: (62, 1) (62, 14) (62, 14, 52)\n",
            "I: 15\n",
            "I after: 15\n",
            "Shapes test: (41, 1) (41, 15) (41, 15, 52)\n",
            "I: 16\n",
            "I after: 16\n",
            "Shapes test: (53, 1) (53, 16) (53, 16, 52)\n",
            "I: 17\n",
            "I after: 17\n",
            "Shapes test: (46, 1) (46, 17) (46, 17, 52)\n",
            "I: 18\n",
            "I after: 18\n",
            "Shapes test: (56, 1) (56, 18) (56, 18, 52)\n",
            "I: 19\n",
            "I after: 19\n",
            "Shapes test: (56, 1) (56, 19) (56, 19, 52)\n",
            "I: 20\n",
            "I after: 20\n",
            "Shapes test: (67, 1) (67, 20) (67, 20, 52)\n",
            "I: 21\n",
            "I after: 21\n",
            "Shapes test: (71, 1) (71, 21) (71, 21, 52)\n",
            "I: 22\n",
            "I after: 22\n",
            "Shapes test: (57, 1) (57, 22) (57, 22, 52)\n",
            "I: 23\n",
            "I after: 23\n",
            "Shapes test: (46, 1) (46, 23) (46, 23, 52)\n",
            "I: 24\n",
            "I after: 24\n",
            "Shapes test: (49, 1) (49, 24) (49, 24, 52)\n",
            "I: 25\n",
            "I after: 25\n",
            "Shapes test: (48, 1) (48, 25) (48, 25, 52)\n",
            "I: 26\n",
            "I after: 26\n",
            "Shapes test: (45, 1) (45, 26) (45, 26, 52)\n",
            "I: 27\n",
            "I after: 27\n",
            "Shapes test: (55, 1) (55, 27) (55, 27, 52)\n",
            "I: 28\n",
            "I after: 28\n",
            "Shapes test: (54, 1) (54, 28) (54, 28, 52)\n",
            "I: 29\n",
            "I after: 29\n",
            "Shapes test: (59, 1) (59, 29) (59, 29, 52)\n",
            "I: 30\n",
            "I after: 30\n",
            "Shapes test: (58, 1) (58, 30) (58, 30, 52)\n",
            "I: 31\n",
            "I after: 31\n",
            "Shapes test: (44, 1) (44, 31) (44, 31, 52)\n",
            "I: 32\n",
            "I after: 32\n",
            "Shapes test: (48, 1) (48, 32) (48, 32, 52)\n",
            "I: 33\n",
            "I after: 33\n",
            "Shapes test: (38, 1) (38, 33) (38, 33, 52)\n",
            "I: 34\n",
            "I after: 34\n",
            "Shapes test: (44, 1) (44, 34) (44, 34, 52)\n",
            "I: 35\n",
            "I after: 35\n",
            "Shapes test: (35, 1) (35, 35) (35, 35, 52)\n",
            "I: 36\n",
            "I after: 36\n",
            "Shapes test: (47, 1) (47, 36) (47, 36, 52)\n",
            "I: 37\n",
            "I after: 37\n",
            "Shapes test: (40, 1) (40, 37) (40, 37, 52)\n",
            "I: 38\n",
            "I after: 38\n",
            "Shapes test: (21, 1) (21, 38) (21, 38, 52)\n",
            "I: 39\n",
            "I after: 39\n",
            "Shapes test: (24, 1) (24, 39) (24, 39, 52)\n",
            "I: 40\n",
            "I after: 40\n",
            "Shapes test: (21, 1) (21, 40) (21, 40, 52)\n",
            "I: 41\n",
            "I after: 41\n",
            "Shapes test: (20, 1) (20, 41) (20, 41, 52)\n",
            "I: 42\n",
            "I after: 42\n",
            "Shapes test: (18, 1) (18, 42) (18, 42, 52)\n",
            "I: 43\n",
            "I after: 43\n",
            "Shapes test: (18, 1) (18, 43) (18, 43, 52)\n",
            "I: 44\n",
            "I after: 44\n",
            "Shapes test: (16, 1) (16, 44) (16, 44, 52)\n",
            "I: 45\n",
            "I after: 45\n",
            "Shapes test: (11, 1) (11, 45) (11, 45, 52)\n",
            "I: 46\n",
            "I after: 46\n",
            "Shapes test: (9, 1) (9, 46) (9, 46, 52)\n",
            "I: 47\n",
            "I after: 47\n",
            "Shapes test: (2, 1) (2, 47) (2, 47, 52)\n",
            "I: 48\n",
            "I after: 48\n",
            "Shapes test: (2, 1) (2, 48) (2, 48, 52)\n",
            "I: 49\n",
            "I after: 49\n",
            "Shapes test: (6, 1) (6, 49) (6, 49, 52)\n",
            "I: 50\n",
            "I after: 50\n",
            "Shapes test: (2, 1) (2, 50) (2, 50, 52)\n",
            "I: 51\n",
            "I after: 51\n",
            "Shapes test: (3, 1) (3, 51) (3, 51, 52)\n",
            "I: 52\n",
            "I after: 52\n",
            "Shapes test: (5, 1) (5, 52) (5, 52, 52)\n",
            "I: 53\n",
            "I after: 53\n",
            "Shapes test: (2, 1) (2, 53) (2, 53, 52)\n",
            "I: 54\n",
            "I after: 54\n",
            "Shapes test: (1, 1) (1, 54) (1, 54, 52)\n",
            "I: 55\n",
            "I after: 55\n",
            "Shapes test: (3, 1) (3, 55) (3, 55, 52)\n",
            "I: 56\n",
            "I after: 56\n",
            "Shapes test: (4, 1) (4, 56) (4, 56, 52)\n",
            "I: 59\n",
            "I after: 59\n",
            "Shapes test: (1, 1) (1, 59) (1, 59, 52)\n",
            "I: 62\n",
            "I after: 62\n",
            "Shapes test: (1, 1) (1, 62) (1, 62, 52)\n",
            "I: 75\n",
            "I after: 75\n",
            "Shapes test: (1, 1) (1, 75) (1, 75, 52)\n",
            "I: 79\n",
            "I after: 79\n",
            "Shapes test: (1, 1) (1, 79) (1, 79, 52)\n",
            "I: 83\n",
            "I after: 83\n",
            "Shapes test: (1, 1) (1, 83) (1, 83, 52)\n",
            "I: 91\n",
            "I after: 91\n",
            "Shapes test: (1, 1) (1, 91) (1, 91, 52)\n",
            "I: 93\n",
            "I after: 93\n",
            "Shapes test: (1, 1) (1, 93) (1, 93, 52)\n",
            "I: 105\n",
            "I after: 105\n",
            "Shapes test: (1, 1) (1, 105) (1, 105, 52)\n",
            "I: 109\n",
            "I after: 109\n",
            "Shapes test: (1, 1) (1, 109) (1, 109, 52)\n",
            "f1 dev  0 \n",
            "\n",
            "Epoch 1/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1f8e2a94c0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b31aa8f321e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;31m#                   print(len(tokens), len(labels))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;31m#                 print(\"Shapes: {} {} {}\".format(tokens.shape, casing.shape, char.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;31m#                 print(\"after train on batch - i: {}\".format(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_b5062mdw19j",
        "colab_type": "code",
        "outputId": "2d61559b-b313-45de-ead6-6f4e9989adc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sentences = [ 'he said further scientific study was required and if it was found that action was needed it should be taken by the european union .'\n",
        " ,'he said no scientific study was required and if it was found that action was needed it should be taken by the european union .']\n",
        "sentences = np.asarray(sentences)\n",
        "# res = tf.reshape(sentences, [-1])\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Reshape:0\", shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "08SgGkR6nMn8",
        "colab_type": "code",
        "outputId": "ddf1b0e2-83ba-468e-ce6b-540b119df007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "embeddings = elmo(tf.squeeze(tf.cast(sentences, tf.string)),\n",
        "signature=\"default\",\n",
        "as_dict=True)[\"elmo\"]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0330 19:08:53.286401 140299740088192 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBTsAMtlAXaO",
        "colab_type": "code",
        "outputId": "3bad8bfe-9aae-40db-e6ef-900a96405438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "tf.squeeze(tf.cast(sentences, tf.string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Squeeze_1:0' shape=(2,) dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "m_f7uhqhnRHz",
        "colab_type": "code",
        "outputId": "e61fcfa6-cf0e-4367-f4ba-23d8000c9849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"module_4_apply_default/aggregation/mul_3:0\", shape=(2, 25, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8HLM7vvqnSgL",
        "colab_type": "code",
        "outputId": "c5cb0cb7-cbeb-4d09-f1cb-42c188f43636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "def load_data(vocab_size, max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size, index_from=INDEX_FROM)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "io8sFwJeTeCA",
        "colab_type": "code",
        "outputId": "974a873e-dfbe-41ef-d89f-d61edf692588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "    x_train,x_test,y_train,y_test = load_data(10000, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEvl1uoITxdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def data_prep_ELMo(train_x, train_y, test_x, test_y, max_len):\n",
        "    INDEX_FROM = 3\n",
        "    train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n",
        "    word_to_index = {k: (v + INDEX_FROM) for k, v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] = 1\n",
        "    word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "    index_to_word = {v: k for k, v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences = []\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "        \n",
        "    print(\"first: {}\".format(' '.join(sentences[0][:max_len])))\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text, train_label, test_text, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr9oMhy-Vehn",
        "colab_type": "code",
        "outputId": "f6bf5ae8-07fe-4706-c3f4-35941a070f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train, y_train,x_test, y_test, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-93b76e06fbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5WAGAqshUKjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFsIAY6EUYWA",
        "colab_type": "code",
        "outputId": "ad24b3d7-f330-4cef-f626-fad61da0bf8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZP5gZkj6R8Kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}