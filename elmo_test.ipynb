{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmo_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon3/deepNN_NER/blob/master/elmo_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r5k3A6IBOlWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6a13333f-1604-4eb8-b8eb-eaf852eb14f8"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 19.4MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.8MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.2MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.4MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.9MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.2MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.9MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.9MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 8.0MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 8.0MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 8.1MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 8.1MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 40.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.8MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.7MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.7MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.7MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.8MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.5MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 8.5MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 8.6MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 8.6MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 8.8MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 42.0MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 46.0MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 48.4MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 42.7MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 41.7MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 48.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 47.9MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 48.3MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 10.2MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 9.9MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 9.9MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 9.8MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 9.8MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 10.0MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 10.0MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 10.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 10.1MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 10.1MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 44.9MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 46.7MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 49.5MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 50.3MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 50.1MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 54.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 55.2MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 54.6MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 55.7MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 56.7MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 58.5MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 62.4MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 62.0MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 61.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 62.4MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 61.7MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 45.0MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 45.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 45.0MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 44.9MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 44.2MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 44.6MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 44.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 44.3MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 44.5MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 42.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 56.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 55.0MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 54.5MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 55.1MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 55.1MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 18.5MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 18.4MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 18.3MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 18.3MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 17.8MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 17.6MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 17.8MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 17.8MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 17.7MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 18.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 50.9MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 52.5MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 53.5MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 52.9MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 61.3MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 65.4MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 63.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 21.8MB/s \n",
            "\u001b[?25h  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QcK_l5YLyjok",
        "colab_type": "code",
        "outputId": "a938f2a4-b7e0-42f5-9fe6-a5b5e5d24cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyxR3hxEOnch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPzHW796AQau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_module = drive.CreateFile({'id':'1lBy0_BsIQ_gzuCkM-P4MWbstIZFkHj8L'})\n",
        "your_module.GetContentFile('preprocess.py')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL1sOrhjkcdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module = drive.CreateFile({'id':'1PV_OV65fPWh82wfuoabTKNdCEMIhkAWN'})\n",
        "prepro_elmo_module.GetContentFile('prepro_elmo.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I6N-2GkA-l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_module = drive.CreateFile({'id': '1nEox1MVwcpP3Fu5RGJzhKi358x9jTA2h'})\n",
        "validation_module.GetContentFile('validation.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yzjtx0HjGRHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate, Lambda\n",
        "from preprocess import readfile, createBatches, iterate_minibatches, addCharInformation, padding\n",
        "from prepro_elmo import createMatrices\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5Nh14J8aDgG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_prep_ELMo(train_x, train_y, test_x, test_y, max_len):\n",
        "    INDEX_FROM = 3\n",
        "    word_to_index = imdb.get_word_index()\n",
        "    word_to_index = {k: (v + INDEX_FROM) for k, v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] = 1\n",
        "    word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "    index_to_word = {v: k for k, v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences = []\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text, train_label, test_text, test_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdX6Eqh-BfKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "16db540a-2098-4578-b097-f89de1c316d5"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 80               # paper: 80\n",
        "DROPOUT = 0.68           # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.0  # not specified in paper, 0.25 recommended; in other papers: 0.0\n",
        "LSTM_STATE_SIZE = 275    # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n",
        "LOCATION_POINTER = '/content/gdrive/My Drive/masters_thesis/'\n",
        "CHAR_EMBEDDING = 30   # paper: 25, previously: 30\n",
        "VERSION = 10\n",
        "EMBEDDING = \"elmo\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0323 12:30:42.635641 140082241873792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1J6puMBBYJ4",
        "colab_type": "code",
        "outputId": "8f38a65f-93d2-4d8b-9be4-7920ca7cabbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        self.char_embedding_size = CHAR_EMBEDDING\n",
        "        self.version = VERSION\n",
        "        self.embedding = EMBEDDING\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(os.path.join(LOCATION_POINTER, \"data/train.txt\"))\n",
        "        self.devSentences = readfile(os.path.join(LOCATION_POINTER, \"data/dev.txt\"))\n",
        "        self.testSentences = readfile(os.path.join(LOCATION_POINTER, \"data/test.txt\"))\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "                    \n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        # creates identity matrix for token cases\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "#         self.wordEmbeddings = []\n",
        "        \n",
        "#         word represented as 50-dim vector\n",
        "#         ToDO: test with 300-dim vectors (GloVE 42B, GloVE 84B)\n",
        "#         if self.embedding == \"fastText\":\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/wiki-news-300d-1M.vec\"), encoding=\"utf-8\")\n",
        "#         else:\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/glove.6B.50d.txt\"), encoding=\"utf-8\")\n",
        "\n",
        "#         # loop through each word in embeddings\n",
        "#         for i, line in enumerate(fEmbeddings):\n",
        "#             if i==0 and self.embedding == \"fastText\":\n",
        "#                 continue\n",
        "                \n",
        "#             split = line.strip().split(\" \") # removes leading and trailing chars and splits into list of single values\n",
        "#             word = split[0]  # embedding word entry\n",
        "\n",
        "#             if len(word2Idx) == 0:  # add padding+unknown\n",
        "#                 word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#                 word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.random.uniform(-0.25, 0.25, len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#             if split[0].lower() in words:\n",
        "#                 vector = np.array([float(num) for num in split[1:]])\n",
        "#                 self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "#                 word2Idx[split[0]] = len(word2Idx)  # corresponding word dict; increments by 1 for each word\n",
        "\n",
        "#         self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx) # 2,3,4 ...\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}  # index to label(reverted)\n",
        "                                                                \n",
        "                                                                \n",
        "    def createBatches(self):\n",
        "        \n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            tokens, casing, char, labels = data\n",
        "            tokens = np.asarray([tokens])\n",
        "            casing = np.asarray([casing])\n",
        "            char = np.asarray([char])\n",
        "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "            correctLabels.append(labels)\n",
        "            predLabels.append(pred)\n",
        "        return predLabels, correctLabels\n",
        "    \n",
        "    \n",
        "\n",
        "    def ELMoEmbedding(self, x):\n",
        "        \"\"\" Return elmo embedding using tf_hub \"\"\"\n",
        "        elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "        # signature points to the purpose of why we would like to use the modules\n",
        "        return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"word_emb\"]\n",
        "   \n",
        "    \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")  #input N sentences, each 52 chras length\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx),self.char_embedding_size, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)  # pool_size=52: max sentence length\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        words_input = Input(shape=(None,), dtype=tf.string, name='words_input')\n",
        "\n",
        "        words = Lambda(self.ELMoEmbedding, output_shape=(1024,))(words_input)\n",
        "#         words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "#         words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self .wordEmbeddings],\n",
        "#                           trainable=False)(words_input)  # trainable=False since we provide word embeddings\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)  ## trainable=False since we provide case embeddings\n",
        "      \n",
        "#         print(\"SIZES FOR CONCATENATION: {} {} {}\".format(casing.output_dim, words.output_dim, embed_char_out.output_dim))\n",
        "        # concat & BLSTM\n",
        "        output = concatenate([words, casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file=os.path.join(LOCATION_POINTER, 'model_{}.png'.format(self.version)))\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            # batch: [word_indices, case_indices, char_indices]\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
        "                labels, tokens, casing,char = batch       \n",
        "                self.model.train_on_batch([tokens, casing,char], labels)\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.char_embedding_size,\n",
        "                                                        self.optimizer.__class__.__name__,\n",
        "                                                        self.version,\n",
        "                                                        self.embedding\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(os.path.join(LOCATION_POINTER, modelName))\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = os.path.join(LOCATION_POINTER, self.modelName + \".txt\")\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "        \n",
        "    def saveResults(self):\n",
        "        plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "        plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"F1 score\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(LOCATION_POINTER, self.modelName + \".png\"))\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0_c8WQ2BnBx",
        "colab_type": "code",
        "outputId": "cabf250a-83c9-4034-961e-77b69164a031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()\n",
        "cnn_blstm.saveResults()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1f8e2a94c0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Construct and run model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcnn_blstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_BLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT_RECURRENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_STATE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONV_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHAR_EMBEDDING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVERSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddCharInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8HLM7vvqnSgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5cb0cb7-cbeb-4d09-f1cb-42c188f43636"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "def load_data(vocab_size, max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size, index_from=INDEX_FROM)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "io8sFwJeTeCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "974a873e-dfbe-41ef-d89f-d61edf692588"
      },
      "cell_type": "code",
      "source": [
        "    x_train,x_test,y_train,y_test = load_data(10000, 200)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEvl1uoITxdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def data_prep_ELMo(train_x, train_y, test_x, test_y, max_len):\n",
        "    INDEX_FROM = 3\n",
        "    train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n",
        "    word_to_index = {k: (v + INDEX_FROM) for k, v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] = 1\n",
        "    word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "    index_to_word = {v: k for k, v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences = []\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "        \n",
        "    print(\"first: {}\".format(' '.join(sentences[0][:max_len])))\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text, train_label, test_text, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr9oMhy-Vehn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "f6bf5ae8-07fe-4706-c3f4-35941a070f0c"
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train, y_train,x_test, y_test, 200)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-93b76e06fbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5WAGAqshUKjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFsIAY6EUYWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ad24b3d7-f330-4cef-f626-fad61da0bf8b"
      },
      "cell_type": "code",
      "source": [
        "print(train_text[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZP5gZkj6R8Kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}