{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmo_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon3/deepNN_NER/blob/master/elmo_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r5k3A6IBOlWe",
        "colab_type": "code",
        "outputId": "232b1dc9-1b1c-4edb-a26f-7903d0240535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 23.6MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 4.5MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 6.4MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 4.2MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 5.1MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 6.0MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 6.8MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 7.5MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 8.2MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 6.7MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 6.8MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.2MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 9.2MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 16.1MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 16.2MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 16.2MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 16.2MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 16.7MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 16.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 39.9MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 20.8MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 20.9MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 20.9MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 20.7MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 20.8MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 19.8MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 20.3MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 20.3MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 20.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 21.5MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 44.3MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 43.6MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 46.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 43.6MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 43.5MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 48.6MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 48.5MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 48.5MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 30.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 29.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 29.8MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 30.3MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 29.6MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 29.7MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 29.7MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 30.0MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 29.8MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 29.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 44.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 44.6MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 44.7MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 45.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 45.6MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 50.0MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 49.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 48.9MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 50.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 51.5MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 52.4MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 55.8MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 54.1MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 53.8MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 54.1MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 51.9MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 40.7MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 40.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 38.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 38.3MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 38.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 38.4MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 38.9MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 38.7MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 38.7MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 39.7MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 51.2MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 52.2MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 54.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 55.1MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 53.9MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 53.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 53.2MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 53.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 54.1MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 48.9MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 47.7MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 48.5MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 49.1MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 49.2MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 50.5MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 50.6MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 50.3MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 49.5MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 48.4MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 54.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 54.7MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 52.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 19.5MB/s \n",
            "\u001b[?25h  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QcK_l5YLyjok",
        "colab_type": "code",
        "outputId": "470c7752-1473-4d61-d0e9-64c35a66fb58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyxR3hxEOnch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPzHW796AQau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_module = drive.CreateFile({'id':'1lBy0_BsIQ_gzuCkM-P4MWbstIZFkHj8L'})\n",
        "your_module.GetContentFile('preprocess.py')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL1sOrhjkcdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module = drive.CreateFile({'id':'1S_fLdw31wQMBtIqIzEIelYlLim4A8dxA'})\n",
        "prepro_elmo_module.GetContentFile('elmo_preprocessing1.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRqpT7ciZY5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module1 = drive.CreateFile({'id':'1jK137fvsVr_DO7UiEA9J0Qbx_LLm7XPS'})\n",
        "prepro_elmo_module1.GetContentFile('elmo_prep11.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I6N-2GkA-l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_module = drive.CreateFile({'id': '1nEox1MVwcpP3Fu5RGJzhKi358x9jTA2h'})\n",
        "validation_module.GetContentFile('validation.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb4L3wr2jq4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module2 = drive.CreateFile({'id':'1gmJJScIBfWmOdDMzOJO2kZvHb-r0zJpG'})\n",
        "prepro_elmo_module2.GetContentFile('elmo_prep4.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yzjtx0HjGRHv",
        "colab_type": "code",
        "outputId": "5bd63c85-21ea-4751-b2ca-13b6aece4a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate, Lambda\n",
        "from preprocess import readfile, addCharInformation, padding\n",
        "from elmo_prep4 import createMatrices, createBatches, iterate_minibatches\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0327 16:56:30.744528 140683540060032 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xdX6Eqh-BfKz",
        "colab_type": "code",
        "outputId": "3807c351-2c1e-4bd2-8954-31c11a6f97e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 80               # paper: 80\n",
        "DROPOUT = 0.68           # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.0  # not specified in paper, 0.25 recommended; in other papers: 0.0\n",
        "LSTM_STATE_SIZE = 275    # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n",
        "LOCATION_POINTER = '/content/gdrive/My Drive/masters_thesis/'\n",
        "CHAR_EMBEDDING = 30   # paper: 25, previously: 30\n",
        "VERSION = 16\n",
        "EMBEDDING = \"elmo\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0327 16:56:33.556960 140683540060032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1J6puMBBYJ4",
        "colab_type": "code",
        "outputId": "93d6e024-8796-47d9-96b5-5ddc91a60db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        self.char_embedding_size = CHAR_EMBEDDING\n",
        "        self.version = VERSION\n",
        "        self.embedding = EMBEDDING\n",
        "#         self.tokens_length = []\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(os.path.join(LOCATION_POINTER, \"data/train.txt\"))\n",
        "        self.devSentences = readfile(os.path.join(LOCATION_POINTER, \"data/dev.txt\"))\n",
        "        self.testSentences = readfile(os.path.join(LOCATION_POINTER, \"data/test.txt\"))\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "                    \n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        # creates identity matrix for token cases\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "#         self.wordEmbeddings = []\n",
        "        \n",
        "#         word represented as 50-dim vector\n",
        "#         ToDO: test with 300-dim vectors (GloVE 42B, GloVE 84B)\n",
        "#         if self.embedding == \"fastText\":\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/wiki-news-300d-1M.vec\"), encoding=\"utf-8\")\n",
        "#         else:\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/glove.6B.50d.txt\"), encoding=\"utf-8\")\n",
        "\n",
        "#         # loop through each word in embeddings\n",
        "#         for i, line in enumerate(fEmbeddings):\n",
        "#             if i==0 and self.embedding == \"fastText\":\n",
        "#                 continue\n",
        "                \n",
        "#             split = line.strip().split(\" \") # removes leading and trailing chars and splits into list of single values\n",
        "#             word = split[0]  # embedding word entry\n",
        "\n",
        "#             if len(word2Idx) == 0:  # add padding+unknown\n",
        "#                 word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#                 word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.random.uniform(-0.25, 0.25, len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#             if split[0].lower() in words:\n",
        "#                 vector = np.array([float(num) for num in split[1:]])\n",
        "#                 self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "#                 word2Idx[split[0]] = len(word2Idx)  # corresponding word dict; increments by 1 for each word\n",
        "\n",
        "#         self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx) # 2,3,4 ...\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}  # index to label(reverted)\n",
        "                                                                \n",
        "                                                                \n",
        "    def createBatches(self):\n",
        "        \n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            tokens, casing, char, labels = data\n",
        "            tokens = np.asarray([tokens])\n",
        "            casing = np.asarray([casing])\n",
        "            char = np.asarray([char])\n",
        "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "            correctLabels.append(labels)\n",
        "            predLabels.append(pred)\n",
        "        return predLabels, correctLabels\n",
        "    \n",
        "    \n",
        "\n",
        "    def ELMoEmbedding(self, tokens_input):\n",
        "        # ToDO: pojedyncze zdanie na inpucie - lista słów (stringi)\n",
        "        #       zdania na inpucie - lista list \n",
        "        # zdefiniować input jako tokeny!\n",
        "        \"\"\" Return elmo embedding using tf_hub \"\"\"\n",
        "        elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "        # signature points to the purpose of why we would like to use the modules\n",
        "        # as_dict=True needed to output word embeddings instead of defualtss\n",
        "        return elmo_model(tf.squeeze(tf.cast(tokens_input, tf.string)), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "#         return elmo_model(inputs={\"tokens\": tokens_input, \"sequence_len\": tokens_length}, signature=\"tokens\", as_dict=True)[\"word_emb\"]\n",
        "#       return elmo_model(x, signature=\"default\", as_dict=True)[\"word_emb\"]\n",
        "  \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")  #input N sentences, each 52 chras length\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx),self.char_embedding_size, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)  # pool_size=52: max sentence length\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        words_input = Input(shape=(None,), dtype=tf.string, name='words_input')\n",
        "        # model takes first dimension as number of examplese and second as the size of each example\n",
        "#         tokens_length = Input(shape=(None, None, ), name='tokens_length_input')  # co z data type\n",
        "\n",
        "        # ToDO: Lambda powinna zwrócić odpowiedni rozmiar!\n",
        "#         words = self.ELMoEmbedding(words_input, tokens_length)\n",
        "        words = Lambda(self.ELMoEmbedding, output_shape=(None, 1024 ))(words_input)\n",
        "#         words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "#         words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self .wordEmbeddings],\n",
        "#                           trainable=False)(words_input)  # trainable=False since we provide word embeddings\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)  ## trainable=False since we provide case embeddings\n",
        "       \n",
        "      \n",
        "#         print(\"SIZES FOR CONCATENATION: {} {} {}\".format(tf.dim(words), tf.dim(char), tf.dim(casing)))\n",
        "        # concat & BLSTM\n",
        "        print(\"DIMENSIONS: {}\".format(tf.shape(words_input)))\n",
        "\n",
        "        output = concatenate([words, casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file=os.path.join(LOCATION_POINTER, 'model_{}.png'.format(self.version)))\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            # batch: [word_indices, case_indices, char_indices]\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
        "                labels, tokens, casing, char, self.tokens_length = batch  \n",
        "                tokens = np.expand_dims(tokens[0], 1)\n",
        "                print(tokens)\n",
        "                print(\"length: {}\".format(len(tokens)))\n",
        "                \n",
        "                self.model.train_on_batch([tokens, casing, char], labels)\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.char_embedding_size,\n",
        "                                                        self.optimizer.__class__.__name__,\n",
        "                                                        self.version,\n",
        "                                                        self.embedding\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(os.path.join(LOCATION_POINTER, modelName))\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = os.path.join(LOCATION_POINTER, self.modelName + \".txt\")\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "        \n",
        "    def saveResults(self):\n",
        "        plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "        plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"F1 score\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(LOCATION_POINTER, self.modelName + \".png\"))\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0_c8WQ2BnBx",
        "colab_type": "code",
        "outputId": "24ba67e9-6a0c-400a-cfa7-d514b396d5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1685
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()\n",
        "cnn_blstm.saveResults()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BEFORE SET LENGTH: 14041\n",
            "SET LENGTH: 301\n",
            "######################## AFTER PROCESSING ############################\n",
            "[1, 19, 29, 58, 67, 123, 155, 215, 287, 359, 456, 522, 604, 663, 766, 918, 1178, 1508, 1769, 2077, 2381, 2668, 2923, 3147, 3316, 3470, 3633, 3806, 3985, 4148, 4321, 4522, 4716, 4912, 5045, 5191, 5328, 5469, 5608, 5751, 5884, 5997, 6110, 6220, 6342, 6446, 6547, 6668, 6775, 6879, 6995, 7124, 7270, 7383, 7459, 7539, 7624, 7714, 7793, 7885, 8001, 8093, 8183, 8303, 8398, 8500, 8562, 8634, 8684, 8721, 8759, 8809, 8848, 8899, 8941, 8971, 9006, 9049, 9100, 9123, 9146, 9193, 9225, 9260, 9293, 9321, 9346, 9383, 9413, 9454, 9486, 9519, 9550, 9573, 9606, 9642, 9685, 9726, 9756, 9796, 9814, 9836, 9873, 9909, 9944, 9981, 10018, 10054, 10099, 10143, 10177, 10205, 10250, 10279, 10325, 10366, 10410, 10451, 10487, 10518, 10560, 10601, 10637, 10671, 10701, 10749, 10790, 10824, 10859, 10891, 10919, 10947, 10971, 11005, 11042, 11079, 11124, 11153, 11197, 11239, 11264, 11300, 11326, 11359, 11404, 11430, 11457, 11498, 11542, 11577, 11613, 11643, 11691, 11722, 11747, 11779, 11810, 11842, 11885, 11916, 11946, 11982, 12011, 12044, 12081, 12111, 12150, 12189, 12225, 12253, 12294, 12326, 12360, 12388, 12417, 12458, 12495, 12535, 12584, 12628, 12669, 12705, 12742, 12776, 12792, 12822, 12840, 12875, 12892, 12915, 12945, 12971, 12994, 13020, 13041, 13065, 13090, 13112, 13133, 13158, 13170, 13189, 13214, 13232, 13249, 13271, 13293, 13312, 13330, 13354, 13370, 13393, 13406, 13426, 13445, 13463, 13482, 13501, 13506, 13525, 13545, 13557, 13571, 13587, 13604, 13615, 13628, 13639, 13655, 13673, 13695, 13715, 13727, 13737, 13749, 13759, 13769, 13787, 13798, 13814, 13821, 13835, 13847, 13857, 13866, 13875, 13885, 13898, 13903, 13909, 13917, 13923, 13926, 13929, 13936, 13938, 13941, 13945, 13950, 13951, 13954, 13957, 13960, 13962, 13963, 13965, 13967, 13970, 13975, 13980, 13984, 13987, 13991, 13992, 13994, 13996, 13998, 13999, 14001, 14002, 14005, 14007, 14008, 14010, 14011, 14013, 14014, 14017, 14022, 14026, 14027, 14029, 14031, 14033, 14034, 14035, 14036, 14037, 14039, 14040, 14041] 14041\n",
            "\n",
            "BEFORE SET LENGTH: 3250\n",
            "SET LENGTH: 280\n",
            "######################## AFTER PROCESSING ############################\n",
            "[4, 6, 8, 11, 26, 32, 49, 60, 84, 106, 118, 133, 160, 204, 280, 339, 406, 478, 550, 609, 670, 711, 753, 787, 833, 867, 900, 925, 949, 988, 1037, 1074, 1103, 1146, 1187, 1215, 1235, 1266, 1279, 1310, 1330, 1353, 1374, 1396, 1417, 1436, 1464, 1486, 1514, 1545, 1571, 1589, 1609, 1640, 1663, 1679, 1705, 1723, 1746, 1762, 1787, 1810, 1824, 1836, 1849, 1862, 1874, 1880, 1888, 1895, 1901, 1909, 1917, 1928, 1939, 1951, 1958, 1964, 1973, 1981, 1993, 2005, 2011, 2020, 2025, 2033, 2041, 2046, 2058, 2069, 2080, 2086, 2099, 2104, 2109, 2118, 2128, 2135, 2146, 2155, 2161, 2171, 2176, 2189, 2199, 2211, 2219, 2224, 2234, 2239, 2254, 2267, 2277, 2295, 2306, 2313, 2322, 2326, 2339, 2346, 2362, 2372, 2378, 2391, 2400, 2409, 2417, 2424, 2430, 2441, 2452, 2470, 2479, 2489, 2496, 2504, 2511, 2518, 2526, 2532, 2540, 2554, 2560, 2571, 2578, 2587, 2592, 2600, 2602, 2609, 2614, 2625, 2638, 2643, 2651, 2658, 2664, 2671, 2682, 2689, 2699, 2708, 2716, 2731, 2741, 2752, 2763, 2768, 2774, 2781, 2787, 2797, 2803, 2813, 2826, 2834, 2841, 2850, 2857, 2867, 2875, 2882, 2896, 2901, 2914, 2921, 2928, 2934, 2937, 2945, 2951, 2959, 2967, 2973, 2981, 2990, 2992, 2996, 3006, 3012, 3016, 3022, 3024, 3034, 3041, 3047, 3051, 3056, 3060, 3067, 3072, 3077, 3084, 3092, 3097, 3099, 3102, 3106, 3108, 3111, 3116, 3122, 3128, 3134, 3135, 3138, 3140, 3142, 3146, 3147, 3151, 3154, 3157, 3160, 3164, 3167, 3171, 3177, 3182, 3185, 3188, 3190, 3193, 3197, 3199, 3202, 3203, 3204, 3208, 3211, 3212, 3214, 3215, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3229, 3231, 3232, 3233, 3235, 3236, 3238, 3239, 3241, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250] 3250\n",
            "\n",
            "BEFORE SET LENGTH: 3453\n",
            "SET LENGTH: 271\n",
            "######################## AFTER PROCESSING ############################\n",
            "[3, 4, 9, 23, 41, 67, 79, 85, 101, 116, 131, 154, 181, 209, 267, 367, 433, 516, 570, 630, 669, 740, 815, 891, 984, 1052, 1125, 1174, 1227, 1279, 1330, 1393, 1454, 1505, 1557, 1608, 1652, 1688, 1718, 1755, 1787, 1811, 1847, 1869, 1886, 1907, 1925, 1947, 1975, 2012, 2050, 2072, 2099, 2115, 2127, 2146, 2160, 2178, 2196, 2208, 2234, 2254, 2264, 2274, 2287, 2297, 2304, 2312, 2317, 2329, 2334, 2337, 2348, 2351, 2359, 2364, 2372, 2383, 2393, 2400, 2410, 2419, 2425, 2431, 2434, 2440, 2450, 2456, 2466, 2470, 2477, 2481, 2491, 2500, 2509, 2516, 2529, 2534, 2540, 2546, 2559, 2564, 2575, 2582, 2593, 2598, 2608, 2619, 2628, 2641, 2647, 2655, 2662, 2671, 2682, 2692, 2704, 2716, 2718, 2731, 2742, 2758, 2767, 2771, 2775, 2780, 2787, 2793, 2800, 2811, 2821, 2829, 2837, 2846, 2851, 2857, 2863, 2872, 2875, 2881, 2890, 2899, 2910, 2914, 2919, 2928, 2930, 2938, 2951, 2957, 2962, 2972, 2976, 2986, 2988, 2997, 3001, 3009, 3016, 3026, 3033, 3035, 3041, 3048, 3058, 3066, 3074, 3081, 3092, 3098, 3108, 3114, 3119, 3126, 3132, 3142, 3148, 3156, 3167, 3175, 3178, 3185, 3191, 3198, 3203, 3209, 3213, 3216, 3221, 3227, 3233, 3236, 3239, 3245, 3249, 3255, 3260, 3266, 3271, 3276, 3278, 3283, 3291, 3296, 3300, 3302, 3306, 3311, 3314, 3317, 3319, 3320, 3323, 3326, 3330, 3334, 3337, 3340, 3348, 3354, 3358, 3362, 3365, 3367, 3370, 3371, 3373, 3375, 3377, 3383, 3387, 3391, 3392, 3394, 3396, 3397, 3399, 3406, 3408, 3411, 3412, 3414, 3416, 3417, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3440, 3441, 3443, 3446, 3448, 3449, 3450, 3451, 3452, 3453] 3453\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0327 16:56:55.705236 140683540060032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0327 16:57:02.418910 140683540060032 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DIMENSIONS: Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n",
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/80\n",
            "['market talk - usda net change in weekly export commitments for the week ended august 22 , includes old crop and new crop , were : wheat up 595,400 tonnes old , nil new ; corn up 1,900 old , up 319,600 new ; soybeans down 12,300 old , up 300,800 new ; upland cotton up 50,400 bales new , nil old ; soymeal 54,800 old , up 100,600 new , soyoil nil old , up 75,000 new ; barley up 1,700 old , nil new ; sorghum 6,200 old , up 156,700 new ; pima cotton up 4,000 bales old , nil new ; rice up 49,900 old , nil new ...']\n",
            "length: 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
            "W0327 16:57:06.795244 140683540060032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1f8e2a94c0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-5bb3510802eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# compute F1 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be a vector, got shape: []\n\t [[{{node lambda_1/module_apply_default/StringSplit}}]]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8HLM7vvqnSgL",
        "colab_type": "code",
        "outputId": "c5cb0cb7-cbeb-4d09-f1cb-42c188f43636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "def load_data(vocab_size, max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size, index_from=INDEX_FROM)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "io8sFwJeTeCA",
        "colab_type": "code",
        "outputId": "974a873e-dfbe-41ef-d89f-d61edf692588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "    x_train,x_test,y_train,y_test = load_data(10000, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEvl1uoITxdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def data_prep_ELMo(train_x, train_y, test_x, test_y, max_len):\n",
        "    INDEX_FROM = 3\n",
        "    train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n",
        "    word_to_index = {k: (v + INDEX_FROM) for k, v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] = 1\n",
        "    word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "    index_to_word = {v: k for k, v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences = []\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "        \n",
        "    print(\"first: {}\".format(' '.join(sentences[0][:max_len])))\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text, train_label, test_text, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr9oMhy-Vehn",
        "colab_type": "code",
        "outputId": "f6bf5ae8-07fe-4706-c3f4-35941a070f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train, y_train,x_test, y_test, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-93b76e06fbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5WAGAqshUKjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFsIAY6EUYWA",
        "colab_type": "code",
        "outputId": "ad24b3d7-f330-4cef-f626-fad61da0bf8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZP5gZkj6R8Kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}