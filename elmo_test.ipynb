{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmo_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon3/deepNN_NER/blob/master/elmo_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r5k3A6IBOlWe",
        "colab_type": "code",
        "outputId": "8c4b9efc-e891-4ee1-ee30-2e9aa61e4593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 20.7MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.9MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.8MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.2MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.4MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.9MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 4.1MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 4.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.7MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.7MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 7.7MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 7.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 7.8MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 7.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 39.8MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.6MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.6MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.6MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.6MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.6MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.4MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 8.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 8.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 8.4MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 8.6MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 41.0MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 41.3MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 42.7MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 39.5MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 41.0MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 46.9MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 47.4MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 47.6MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 10.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 9.9MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 9.8MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 9.8MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 9.7MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 9.8MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 9.8MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 9.7MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 9.7MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 9.7MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 41.1MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 40.0MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 41.6MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 43.1MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 45.3MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 48.7MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 48.2MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 47.9MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 47.4MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 46.7MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 47.3MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 50.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 50.7MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 50.7MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 49.8MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 49.8MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 39.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 39.7MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 40.0MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 40.5MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 41.0MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 41.4MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 40.8MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 40.9MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 15.5MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 15.4MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 16.6MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 16.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 16.6MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 16.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 16.4MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 16.4MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 16.5MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 16.5MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 48.7MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 44.7MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 46.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 46.0MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 46.0MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.9MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 48.3MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 48.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 47.7MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 46.8MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 47.8MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 49.5MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 48.9MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 50.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 18.4MB/s \n",
            "\u001b[?25h  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QcK_l5YLyjok",
        "colab_type": "code",
        "outputId": "1896791c-7be2-4524-f063-03c3639f5c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FyxR3hxEOnch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPzHW796AQau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_module = drive.CreateFile({'id':'1lBy0_BsIQ_gzuCkM-P4MWbstIZFkHj8L'})\n",
        "your_module.GetContentFile('preprocess.py')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SL1sOrhjkcdZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module = drive.CreateFile({'id':'1S_fLdw31wQMBtIqIzEIelYlLim4A8dxA'})\n",
        "prepro_elmo_module.GetContentFile('elmo_preprocessing1.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRqpT7ciZY5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module1 = drive.CreateFile({'id':'1jK137fvsVr_DO7UiEA9J0Qbx_LLm7XPS'})\n",
        "prepro_elmo_module1.GetContentFile('elmo_prep11.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_I6N-2GkA-l3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_module = drive.CreateFile({'id': '1nEox1MVwcpP3Fu5RGJzhKi358x9jTA2h'})\n",
        "validation_module.GetContentFile('validation.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb4L3wr2jq4c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module2 = drive.CreateFile({'id':'1gmJJScIBfWmOdDMzOJO2kZvHb-r0zJpG'})\n",
        "prepro_elmo_module2.GetContentFile('elmo_prep4.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QdmP1G75z_ED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prepro_elmo_module3 = drive.CreateFile({'id':'1v9lb4SlclxAAKYjIRk0lrYp5oP3WEU3J'})\n",
        "prepro_elmo_module3.GetContentFile('elmo_preprocess_sentences_nontrain.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yzjtx0HjGRHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load packages\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from validation import compute_f1\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate, Lambda\n",
        "#from preprocess import readfile, addCharInformation, padding\n",
        "from elmo_preprocess_sentences_nontrain import createMatrices, createBatches, createBatchesNonTrain, iterate_minibatches, readfile, addCharInformation, padding\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdX6Eqh-BfKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 80               # paper: 80\n",
        "DROPOUT = 0.68           # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.0  # not specified in paper, 0.25 recommended; in other papers: 0.0\n",
        "LSTM_STATE_SIZE = 275    # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended\n",
        "LOCATION_POINTER = '/content/gdrive/My Drive/masters_thesis/'\n",
        "CHAR_EMBEDDING = 30   # paper: 25, previously: 30\n",
        "VERSION = 16\n",
        "EMBEDDING = \"elmo\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1J6puMBBYJ4",
        "colab_type": "code",
        "outputId": "89394c16-b1e4-400a-e633-611f294f26c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        self.char_embedding_size = CHAR_EMBEDDING\n",
        "        self.version = VERSION\n",
        "        self.embedding = EMBEDDING\n",
        "#         self.tokens_length = []\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(os.path.join(LOCATION_POINTER, \"data/train.txt\"))\n",
        "        self.devSentences = readfile(os.path.join(LOCATION_POINTER, \"data/dev.txt\"))\n",
        "        self.testSentences = readfile(os.path.join(LOCATION_POINTER, \"data/test.txt\"))\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "                    \n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        # creates identity matrix for token cases\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "#         self.wordEmbeddings = []\n",
        "        \n",
        "#         word represented as 50-dim vector\n",
        "#         ToDO: test with 300-dim vectors (GloVE 42B, GloVE 84B)\n",
        "#         if self.embedding == \"fastText\":\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/wiki-news-300d-1M.vec\"), encoding=\"utf-8\")\n",
        "#         else:\n",
        "#           fEmbeddings = open(os.path.join(LOCATION_POINTER, \"embeddings/glove.6B.50d.txt\"), encoding=\"utf-8\")\n",
        "\n",
        "#         # loop through each word in embeddings\n",
        "#         for i, line in enumerate(fEmbeddings):\n",
        "#             if i==0 and self.embedding == \"fastText\":\n",
        "#                 continue\n",
        "                \n",
        "#             split = line.strip().split(\" \") # removes leading and trailing chars and splits into list of single values\n",
        "#             word = split[0]  # embedding word entry\n",
        "\n",
        "#             if len(word2Idx) == 0:  # add padding+unknown\n",
        "#                 word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#                 word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "#                 vector = np.random.uniform(-0.25, 0.25, len(split) - 1)  # zero vector for 'PADDING' word\n",
        "#                 self.wordEmbeddings.append(vector)\n",
        "\n",
        "#             if split[0].lower() in words:\n",
        "#                 vector = np.array([float(num) for num in split[1:]])\n",
        "#                 self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "#                 word2Idx[split[0]] = len(word2Idx)  # corresponding word dict; increments by 1 for each word\n",
        "\n",
        "#         self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx) # 2,3,4 ...\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}  # index to label(reverted)\n",
        "                                                                \n",
        "                                                                \n",
        "    def createBatches(self):\n",
        "        \n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatchesNonTrain(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatchesNonTrain(self.test_set)\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            tokens, casing, char, labels, _ = data\n",
        "            if len(tokens)<=1:\n",
        "              continue\n",
        "            tokens = np.asarray([tokens])\n",
        "            print(\"TAG TOKENS: {}\".format(tokens))\n",
        "            casing = np.asarray([casing])\n",
        "            char = np.asarray([char])\n",
        "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "            correctLabels.append(labels)\n",
        "            predLabels.append(pred)\n",
        "        return predLabels, correctLabels\n",
        "#         correctLabels = []\n",
        "#         predLabels = []\n",
        "#         # ToDO: only training data as sentences\n",
        "#         # ToDO: change for iteration through dict!\n",
        "#         for i in dataset.keys():\n",
        "#           key = str(i)\n",
        "#           data = dataset[key]\n",
        "#           for dt in data:\n",
        "#             ################################################################\n",
        "#             # sentences or tokens here!?\n",
        "#             ################################################################\n",
        "#             tokens, casing, char, labels, _ = dt\n",
        "# #             tokens = np.asarray([tokens])\n",
        "#             casing = np.asarray([casing])\n",
        "#             char = np.asarray([char])\n",
        "#             print(\"TAGGED tokens: {}\".format(tokens))\n",
        "#             pred = model.predict([tokens, casing, char], verbose=False, batch_size=len(tokens))[0]\n",
        "#             pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "#             correctLabels.append(labels)\n",
        "#             predLabels.append(pred)\n",
        "#         return predLabels, correctLabels\n",
        "    \n",
        "    \n",
        "\n",
        "    def ELMoEmbedding(self, tokens_input):\n",
        "        # ToDO: pojedyncze zdanie na inpucie - lista słów (stringi)\n",
        "        #       zdania na inpucie - lista list \n",
        "        # zdefiniować input jako tokeny!\n",
        "        \"\"\" Return elmo embedding using tf_hub \"\"\"\n",
        "        elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "        # signature points to the purpose of why we would like to use the modules\n",
        "        # as_dict=True needed to output word embeddings instead of defualtss\n",
        "#         return elmo_model(tf.cast(tokens_input, tf.string), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "#         return elmo_model(tokens_input, signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "        return elmo_model(tf.squeeze(tf.cast(tokens_input, tf.string)), signature=\"default\", as_dict=True)['elmo']  #[\"word_emb\"]  #[\"default\"]  #[\"word_emb\"]\n",
        "\n",
        "#         return elmo_model(inputs={\"tokens\": tokens_input, \"sequence_len\": tokens_length}, signature=\"tokens\", as_dict=True)[\"word_emb\"]\n",
        "#       return elmo_model(x, signature=\"default\", as_dict=True)[\"word_emb\"]\n",
        "  \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")  #input N sentences, each 52 chras length\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx),self.char_embedding_size, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)  # pool_size=52: max sentence length\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        # input of N-dimensional vectors (1st arg in shape points to the size of input vectors)\n",
        "        words_input = Input(shape=(None,), dtype=tf.string, name='words_input')\n",
        "        # model takes first dimension as number of examplese and second as the size of each example\n",
        "#         tokens_length = Input(shape=(None, None, ), name='tokens_length_input')  # co z data type\n",
        "        print(\"After words input\")\n",
        "        # ToDO: Lambda powinna zwrócić odpowiedni rozmiar!\n",
        "#         words = self.ELMoEmbedding(words_input, tokens_length)\n",
        "        words = Lambda(self.ELMoEmbedding, output_shape=(None, 1024))(words_input)\n",
        "#         words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "#         words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self .wordEmbeddings],\n",
        "#                           trainable=False)(words_input)  # trainable=False since we provide word embeddings\n",
        "        print(\"After: {}\".format(words))\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)  ## trainable=False since we provide case embeddings\n",
        "       \n",
        "      \n",
        "#         print(\"SIZES FOR CONCATENATION: {} {} {}\".format(tf.dim(words), tf.dim(char), tf.dim(casing)))\n",
        "        # concat & BLSTM\n",
        "        print(\"DIMENSIONS: {}\".format(tf.shape(words_input)))\n",
        "              \n",
        "        output = concatenate([words, casing, char])\n",
        "#         output = concatenate([tf.reshape(words, \n",
        "#                                   [len(words), 52, 1024]), casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        for layer in self.model.layers:\n",
        "            print(\"Layer {}: {}\".format(layer.name, layer.output_shape))\n",
        "        \n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file=os.path.join(LOCATION_POINTER, 'model_{}.png'.format(self.version)))\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            # batch: [word_indices, case_indices, char_indices]\n",
        "            print(\"Batch len: {}\".format(self.train_batch_len))\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch, self.train_batch_len)):\n",
        "                labels, tokens, casing, char, self.tokens_length = batch  \n",
        "#                 tokens = np.expand_dims(tokens[0], 1)\n",
        "#                 print(tokens)\n",
        "#                 print(\"length: {}\".format(len(tokens)))\n",
        "#                 print(\"TOKENS: {}\".format(tokens))\n",
        "#                 tokens_in = tf.reshape(tokens, [-1])\n",
        "#                 print(labels)\n",
        "#                 print(tokens[:5])\n",
        "#                 print(\"before train on batch - i: {}\".format(i))\n",
        "                if len(tokens) <= 1:\n",
        "                  continue\n",
        "#                 if i==55 or i ==2:\n",
        "#                   print(len(tokens), len(labels))\n",
        "                self.model.train_on_batch([tokens, casing, char], labels)\n",
        "#                 print(\"after train on batch - i: {}\".format(i))\n",
        "\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.char_embedding_size,\n",
        "                                                        self.optimizer.__class__.__name__,\n",
        "                                                        self.version,\n",
        "                                                        self.embedding\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(os.path.join(LOCATION_POINTER, modelName))\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = os.path.join(LOCATION_POINTER, self.modelName + \".txt\")\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "        \n",
        "    def saveResults(self):\n",
        "        plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "        plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"F1 score\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(LOCATION_POINTER, self.modelName + \".png\"))\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0_c8WQ2BnBx",
        "colab_type": "code",
        "outputId": "fd8d821f-71eb-42a6-c36c-6842d0e5bb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1721
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER, CHAR_EMBEDDING, VERSION, EMBEDDING)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()\n",
        "cnn_blstm.saveResults()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After words input\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0331 16:10:21.644782 140154618275712 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "After: Tensor(\"lambda_9/module_8_apply_default/aggregation/mul_3:0\", shape=(?, ?, 1024), dtype=float32)\n",
            "DIMENSIONS: Tensor(\"Shape_8:0\", shape=(2,), dtype=int32)\n",
            "Layer Character_input: (None, None, 52)\n",
            "Layer Character_embedding: (None, None, 52, 30)\n",
            "Layer dropout_17: (None, None, 52, 30)\n",
            "Layer Convolution: (None, None, 52, 30)\n",
            "Layer Maxpool: (None, None, 1, 30)\n",
            "Layer words_input: (None, None)\n",
            "Layer casing_input: (None, None)\n",
            "Layer Flatten: (None, None, 30)\n",
            "Layer lambda_9: (None, None, 1024)\n",
            "Layer embedding_18: (None, None, 8)\n",
            "Layer dropout_18: (None, None, 30)\n",
            "Layer concatenate_9: (None, None, 1062)\n",
            "Layer BLSTM: (None, None, 550)\n",
            "Layer Softmax_layer: (None, None, 9)\n",
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/80\n",
            "Batch len: dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '62', '67', '78', '113'])\n",
            "TAG TOKENS: [['1-0' '.']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1f8e2a94c0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcnn_blstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-2cc3d3e9e57b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# compute F1 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mpredLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrectLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mpre_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrectLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2Label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_test_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-2cc3d3e9e57b>\u001b[0m in \u001b[0;36mtag_dataset\u001b[0;34m(self, dataset, model)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mcasing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcasing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Predict the classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcorrectLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [2,1,1024] vs. shape[1] = [1,2,8]\n\t [[{{node concatenate_9/concat}}]]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_b5062mdw19j",
        "colab_type": "code",
        "outputId": "2d61559b-b313-45de-ead6-6f4e9989adc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "sentences = [ 'he said further scientific study was required and if it was found that action was needed it should be taken by the european union .'\n",
        " ,'he said no scientific study was required and if it was found that action was needed it should be taken by the european union .']\n",
        "sentences = np.asarray(sentences)\n",
        "# res = tf.reshape(sentences, [-1])\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Reshape:0\", shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "08SgGkR6nMn8",
        "colab_type": "code",
        "outputId": "ddf1b0e2-83ba-468e-ce6b-540b119df007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "embeddings = elmo(tf.squeeze(tf.cast(sentences, tf.string)),\n",
        "signature=\"default\",\n",
        "as_dict=True)[\"elmo\"]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0330 19:08:53.286401 140299740088192 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBTsAMtlAXaO",
        "colab_type": "code",
        "outputId": "3bad8bfe-9aae-40db-e6ef-900a96405438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "tf.squeeze(tf.cast(sentences, tf.string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Squeeze_1:0' shape=(2,) dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "m_f7uhqhnRHz",
        "colab_type": "code",
        "outputId": "e61fcfa6-cf0e-4367-f4ba-23d8000c9849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"module_4_apply_default/aggregation/mul_3:0\", shape=(2, 25, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8HLM7vvqnSgL",
        "colab_type": "code",
        "outputId": "c5cb0cb7-cbeb-4d09-f1cb-42c188f43636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "def load_data(vocab_size, max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size, index_from=INDEX_FROM)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "io8sFwJeTeCA",
        "colab_type": "code",
        "outputId": "974a873e-dfbe-41ef-d89f-d61edf692588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "    x_train,x_test,y_train,y_test = load_data(10000, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEvl1uoITxdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def data_prep_ELMo(train_x, train_y, test_x, test_y, max_len):\n",
        "    INDEX_FROM = 3\n",
        "    train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n",
        "    word_to_index = {k: (v + INDEX_FROM) for k, v in word_to_index.items()}\n",
        "\n",
        "    word_to_index[\"<START>\"] = 1\n",
        "    word_to_index[\"<UNK>\"] = 2\n",
        "\n",
        "    index_to_word = {v: k for k, v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(train_x)):\n",
        "        temp = [index_to_word[ids] for ids in train_x[i]]\n",
        "        sentences.append(temp)\n",
        "\n",
        "    test_sentences = []\n",
        "    for i in range(len(test_x)):\n",
        "        temp = [index_to_word[ids] for ids in test_x[i]]\n",
        "        test_sentences.append(temp)\n",
        "\n",
        "        \n",
        "    print(\"first: {}\".format(' '.join(sentences[0][:max_len])))\n",
        "    train_text = [' '.join(sentences[i][:max_len]) for i in range(len(sentences))]\n",
        "    train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "    train_label = train_y.tolist()\n",
        "\n",
        "    test_text = [' '.join(test_sentences[i][:500]) for i in range(len(test_sentences))]\n",
        "    test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "    test_label = test_y.tolist()\n",
        "\n",
        "    return train_text, train_label, test_text, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr9oMhy-Vehn",
        "colab_type": "code",
        "outputId": "f6bf5ae8-07fe-4706-c3f4-35941a070f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train, y_train,x_test, y_test, 200)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-93b76e06fbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-24-bdac3fad809f>\u001b[0m in \u001b[0;36mdata_prep_ELMo\u001b[0;34m(train_x, train_y, test_x, test_y, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mINDEX_FROM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_prep_ELMo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mINDEX_FROM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5WAGAqshUKjI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text,train_label,test_text,test_label = data_prep_ELMo(x_train,y_train,x_test,y_test,200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFsIAY6EUYWA",
        "colab_type": "code",
        "outputId": "ad24b3d7-f330-4cef-f626-fad61da0bf8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZP5gZkj6R8Kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}